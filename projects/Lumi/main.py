from fastapi import FastAPI, Depends, HTTPException
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
import multiprocessing
import re

import models
from services.search_service import get_search_service, SearchService
from services.ollama_service import OllamaService, get_ollama_service
from services.rag_service import RAGService, get_rag_service

app = FastAPI()

# ----- CORS ì„¤ì • -----
origins = ["*"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ----- í—¬ìŠ¤ ì²´í¬ -----
@app.get("/api/health", response_model=models.HealthStatus)
async def health_check(
    search: SearchService = Depends(get_search_service),
    ollama: OllamaService = Depends(get_ollama_service),
):
    try:
        ollama_status_dict = await ollama.check_ollama_health()
    except Exception as e:
        ollama_status_dict = {"status": "NOT_FOUND", "detail": str(e)}

    try:
        everything_status_dict = search.check_es_health()
    except Exception as e:
        everything_status_dict = {"status": "NOT_FOUND", "detail": str(e)}

    return models.HealthStatus(
        ollama_status=models.HealthStatusDetail(**ollama_status_dict),
        everything_status=models.HealthStatusDetail(**everything_status_dict),
    )

# ----- Ollama ì§ˆì˜ -----
@app.post("/api/ask", response_model=models.AskResponse)
async def ask_ollama(request: models.AskRequest, ollama: OllamaService = Depends(get_ollama_service)):
    try:
        response_text = await ollama.ask(request.prompt)
        return models.AskResponse(response=response_text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ----- Everything ê²€ìƒ‰ -----
@app.post("/api/search", response_model=models.SearchResponse)
def search_files(request: models.SearchRequest, search: SearchService = Depends(get_search_service)):
    try:
        results_list = search.search(request.query)
        return models.SearchResponse(results=results_list)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ----- ë¬¸ì„œ ì²˜ë¦¬ -----
@app.post("/api/process_document", response_model=models.ProcessResponse)
def process_document(request: models.ProcessRequest, rag: RAGService = Depends(get_rag_service)):
    try:
        content, file_format = rag.extract_text(request.file_path)
        return models.ProcessResponse(content=content, format=file_format)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ----- [í•µì‹¬] íŒŒì¼ ê¸°ë°˜ RAG ì±„íŒ… -----
class ChatFileRequest(models.AskRequest):
    file_path: str

@app.post("/api/chat_with_file", response_model=models.AskResponse)
async def chat_with_file(
    request: ChatFileRequest,
    rag: RAGService = Depends(get_rag_service),
    ollama: OllamaService = Depends(get_ollama_service),
):
    try:
        file_content, _ = rag.extract_text(request.file_path)
        truncated_content = file_content[:10000]
        system_prompt = (
            f"You are a helpful assistant. Answer based on the file content.\n\n"
            f"--- File Content ---\n{truncated_content}\n--------------------\n"
        )
        full_prompt = f"{system_prompt}\n\nUser Question: {request.prompt}"
        response_text = await ollama.ask(full_prompt)
        return models.AskResponse(response=response_text)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


# ----- [ìµœì¢… ìˆ˜ì •] AI ì—ì´ì „íŠ¸ (í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹) -----

@app.post("/api/agent", response_model=models.AgentResponse)
async def agent_action(
    request: models.AgentRequest,
    search: SearchService = Depends(get_search_service),
    ollama: OllamaService = Depends(get_ollama_service),
):
    user_q = request.user_query.strip()
    
    # ---------------------------------------------------------
    # [1ë‹¨ê³„] ê²½ë¡œ ì¶”ì¶œ (Path Extraction) - í† í° ê¸°ë°˜ ë°©ì‹ (New)
    # ì •ê·œì‹ ëŒ€ì‹  ì–´ì ˆ(ë„ì–´ì“°ê¸°) ë‹¨ìœ„ë¡œ ë¶„ì„í•˜ì—¬ í•œê¸€ ê²½ë¡œë„ ì™„ë²½í•˜ê²Œ ì¡ìŠµë‹ˆë‹¤.
    # ---------------------------------------------------------
    
    # 1. í•œêµ­ì–´ ì¡°ì‚¬ ë° ë¶ˆí•„ìš”í•œ ë‹¨ì–´ ì œê±° (ê²½ë¡œì— ë¶™ì–´ìˆì„ ìˆ˜ ìˆìŒ)
    # ì£¼ì˜: "í´ë”"ë¼ëŠ” ë‹¨ì–´ëŠ” ê²½ë¡œì˜ ì¼ë¶€ì¼ ìˆ˜ë„ ìˆìœ¼ë‹ˆ(ì˜ˆ: New Folder),
    #       ë‹¨ë…ìœ¼ë¡œ ì“°ì¸ í•œê¸€ "í´ë”"ë§Œ ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ì œê±°í•˜ê±°ë‚˜,
    #       ì•„ì˜ˆ ì œê±°í•˜ì§€ ì•Šê³  "í´ë”"ê°€ ê²½ë¡œ ë’¤ì— ë¶™ì–´ìˆì§€ ì•Šê²Œ ë„ì–´ì“°ê¸°ë¥¼ ë³´ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
    #       ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ 'ë“œë¼ì´ë¸Œ'ì™€ ì¡°ì‚¬ 'ì—ì„œ' ì •ë„ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    clean_q = user_q.replace("ë“œë¼ì´ë¸Œ", ":") \
                    .replace("ì—ì„œ", " ") \
                    .replace("ì— ", " ") # 'ì—'ëŠ” ë’¤ì— ê³µë°±ì´ ìˆì„ ë•Œë§Œ ì¡°ì‚¬ë¡œ ê°„ì£¼
    
    tokens = clean_q.split() # ë„ì–´ì“°ê¸° ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë¶„ë¦¬
    path = None
    
    for token in tokens:
        # "C:", "D:\Work", "E:\ë¯¼í˜" ë“± ë“œë¼ì´ë¸Œ ë¬¸ìë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ì–´ë¥¼ ì°¾ìŒ
        # (ìœˆë„ìš° ê²½ë¡œëŠ” ëŒ€ì†Œë¬¸ì êµ¬ë¶„ì´ ì—†ìœ¼ë¯€ë¡œ ì •ê·œì‹ìœ¼ë¡œ íŒ¨í„´ í™•ì¸)
        if re.match(r'^[a-zA-Z]:', token):
            # ì°¾ì€ í† í°ì„ ê²½ë¡œë¡œ ì§€ì • (ë”°ì˜´í‘œ ì œê±°)
            path = token.replace('"', '').replace("'", "")
            break

    # ---------------------------------------------------------
    # [2ë‹¨ê³„] ì˜ë„(Intent) íŒŒì•… ë° ì¿¼ë¦¬ ì¡°ë¦½
    # ---------------------------------------------------------
    query_str = None
    sort_mode = 0 # ê¸°ë³¸(ì •í™•ë„ìˆœ)
    
    user_q_lower = user_q.lower()
    
    # ê¸°ë³¸ ê²½ë¡œ ë¬¸ìì—´ (ê²½ë¡œê°€ ìˆìœ¼ë©´ ë”°ì˜´í‘œë¡œ ê°ìŒˆ)
    base_path = f'\"{path}\"' if path else ""

    # (A) ë¹ˆ í´ë” ì°¾ê¸°
    if any(w in user_q_lower for w in ["ë¹ˆ ", "ë¹„ì–´ìˆëŠ”", "empty"]):
        # ì˜ˆ: "E:\ë¯¼í˜" folder:childcount:0
        query_str = f'{base_path} folder:childcount:0'.strip()
        sort_mode = 0 
    
    # (B) ìš©ëŸ‰ì´ í° íŒŒì¼
    elif any(w in user_q_lower for w in ["í°", "ë§ì€", "large", "biggest", "highest", "ìš©ëŸ‰"]):
        # ì˜ˆ: "C:" file: (ì •ë ¬ì€ ì½”ë“œì—ì„œ sizeDesc)
        query_str = f'{base_path} file:'.strip()
        sort_mode = 6 # Size Descending
        
    # (C) ìµœê·¼ ìˆ˜ì •ëœ íŒŒì¼
    elif any(w in user_q_lower for w in ["ìµœê·¼", "recent", "ì˜¤ëŠ˜", "today", "ë°©ê¸ˆ", "newest"]):
        if any(w in user_q_lower for w in ["ì˜¤ëŠ˜", "today"]):
            query_str = f'{base_path} dm:today file:'.strip()
        else:
            query_str = f'{base_path} file:'.strip()
        sort_mode = 14 # Date Modified Descending

    # ---------------------------------------------------------
    # [3ë‹¨ê³„] AI Fallback (ê·œì¹™ì— ì•ˆ ê±¸ë¦¬ëŠ” ë³µì¡í•œ ìš”ì²­)
    # ---------------------------------------------------------
    if not query_str:
        system_prompt = (
            "Translate the user's request into an 'Everything' search query.\n"
            "Output ONLY the query inside <query> tags.\n"
            "Rules:\n"
            "1. Wrap paths in double quotes (e.g. \"C:\\Work\").\n"
            "2. Do not include explanations.\n"
            "Examples:\n"
            "- 'Project excel files': <query>project ext:xlsx</query>\n"
            "- 'Files in D:\\Work': <query>\"D:\\Work\" file:</query>\n"
            f"User: {user_q}"
        )
        llm_resp = await ollama.ask(system_prompt)
        
        match = re.search(r"<query>(.*?)</query>", llm_resp, re.DOTALL)
        if match:
            query_str = match.group(1).strip()
        elif any(k in llm_resp for k in ["ext:", "size:", "file:", "folder:", ":\\"]):
            query_str = llm_resp.strip()

    # ---------------------------------------------------------
    # [4ë‹¨ê³„] ê²€ìƒ‰ ì‹¤í–‰
    # ---------------------------------------------------------
    if query_str:
        # ë°±í‹± ë“± ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
        query_str = query_str.replace("`", "").strip()
        
        try:
            results = search.search(query_str, max_results=20, sort_mode=sort_mode)
            
            if not results:
                # ê²°ê³¼ê°€ ì—†ì„ ë•Œ ë””ë²„ê¹…í•˜ê¸° ì¢‹ê²Œ ì¿¼ë¦¬ë¥¼ ë³´ì—¬ì¤Œ
                msg = f"ğŸ” ê²€ìƒ‰ì–´ '{query_str}' (ì •ë ¬: {sort_mode})ë¡œ ì°¾ì•˜ìœ¼ë‚˜ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
            else:
                msg = f"ğŸ” '{query_str}' ì¡°ê±´ìœ¼ë¡œ {len(results)}ê°œì˜ íŒŒì¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤."

            return models.AgentResponse(
                message=msg,
                action_type="search",
                results=[models.SearchResultItem(**r) for r in results]
            )
        except Exception as e:
            return models.AgentResponse(message=f"ê²€ìƒ‰ ì˜¤ë¥˜: {e}", action_type="chat")

    return models.AgentResponse(message="ì£„ì†¡í•©ë‹ˆë‹¤. ê²€ìƒ‰ ëª…ë ¹ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", action_type="chat")


if __name__ == "__main__":
    multiprocessing.freeze_support()
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        reload=False,
        workers=1,
        log_config=None,
    )
